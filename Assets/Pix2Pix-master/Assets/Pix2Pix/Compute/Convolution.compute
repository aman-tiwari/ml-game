// Encoder convolution and decoder transposed convolution kernels
// https://github.com/keijiro/Pix2Pix

#pragma enable_d3d11_debug_symbols

#pragma kernel Conv2D_64 THREADS=64 CACHESIZE=64
#pragma kernel Conv2D_128 THREADS=128 CACHESIZE=128
#pragma kernel Conv2D_256 THREADS=256 CACHESIZE=256
#pragma kernel Conv2D_512 THREADS=512 CACHESIZE=512

#pragma kernel TransConv2D_64 THREADS=64 CACHESIZE=256 TRANSPOSED=1
#pragma kernel TransConv2D_128 THREADS=128 CACHESIZE=512 TRANSPOSED=1
#pragma kernel TransConv2D_256 THREADS=256 CACHESIZE=1024 TRANSPOSED=1
#pragma kernel TransConv2D_512 THREADS=512 CACHESIZE=1024 TRANSPOSED=1
#pragma kernel TransConv2D_final THREADS=32 CACHESIZE=128 TRANSPOSED=1 FINAL=1

Buffer<float> Input;
Buffer<float> Filter;
Buffer<float> Bias;
RWBuffer<float> Output;

uint3 InputShape;
uint4 FilterShape;
uint3 OutputShape;

uint3 InputIndexer;
uint4 FilterIndexer;
uint3 OutputIndexer;

groupshared float cache[CACHESIZE];

float GetInput(uint3 pos, uint2 pad)
{
    // Although it's thought that branching here doesn't make any penalty,
    // some benchmark indicated that it's less performant than masking.
    // So we use masking at the moment. Further investigation should be done
    // as it could make a fair amount of performance gain.
#ifdef PREFER_BRANCHING
    if (any(pos.xy < pad) || any(pos.xy >= InputShape.xy + pad.xy)) return 0;
    pos.xy -= pad;
    return Input[dot(pos, InputIndexer)];
#else
    float mask = 1 - (any(pos.xy < pad) || any(pos.xy >= InputShape.xy + pad));
    pos.xy -= pad * mask;
    return Input[dot(pos, InputIndexer)] * mask;
#endif
}

float GetFilter(uint4 i)
{
    return Filter[dot(i, FilterIndexer)];
}

void StoreOutput(uint3 i, float v)
{
    Output[dot(i, OutputIndexer)] = v;
}

[numthreads(THREADS, 1, 1)]

#define KERNEL_NAME(x, y) x##_##y

#if !defined(TRANSPOSED)
void KERNEL_NAME(Conv2D, THREADS)
#elif !defined(FINAL)
void KERNEL_NAME(TransConv2D, THREADS)
#else
void KERNEL_NAME(TransConv2D, final)
#endif

(const uint3 tid : SV_DispatchThreadID)
{
#if !defined(TRANSPOSED)

    //
    // Normal 2D convolution
    //

    const uint FilterSize = 4; // We assume FilterShape.xy == (4, 4)
    const uint InputChannels = InputShape.z;

    // pos - pad = (upper left corner)
    const uint2 pos = tid.zy * 2;
    const uint pad = FilterSize / 2 - 1;

    float prod = 0;

    for (uint fy = 0; fy < FilterSize; fy++)
    {
        for (uint fx = 0; fx < FilterSize; fx++)
        {
            // Cache the input channel values in a memory coalescing fashion.
            if (tid.x < InputChannels)
                cache[tid.x] = GetInput(uint3(pos + uint2(fy, fx), tid.x), pad);

            GroupMemoryBarrierWithGroupSync();

            // Calculate the product with the filter. This is also expected to
            // run in a memory coalescing fashion.
            for (uint ic = 0; ic < InputChannels; ic++)
                prod += GetFilter(int4(fy, fx, ic, tid.x)) * cache[ic];

            GroupMemoryBarrierWithGroupSync();
        }
    }

    // Output with adding the bias.
    StoreOutput(tid.zyx, prod + Bias[tid.x]);

#else

    //
    // Transposed 2D convolution
    //

    const uint FilterSize = 4; // We assume FilterShape.xy == (4, 4)
    const uint InputChannels = InputShape.z;
    const uint OutputChannels = OutputShape.z;

    float prod = 0;

    for (uint fy = 0; fy < FilterSize; fy += 2)
    {
        for (uint fx = 0; fx < FilterSize; fx += 2)
        {
            uint ic;

            // Actually (tid.zy & 1) should be added to (fy, fx) but we avoid
            // it to prevent the loop counters depending the thread IDs.
            // So we have to recalculate them here.
            const uint2 fyx = uint2(fy, fx) + (tid.zy & 1);

            // Cache the input channel values in a memory coalescing fashion.
            const uint2 iyx = (tid.zy + fyx) / 2;
        #ifdef FINAL
            cache[tid.x     ] = GetInput(uint3(iyx, tid.x     ), 1);
            cache[tid.x + 32] = GetInput(uint3(iyx, tid.x + 32), 1);
            cache[tid.x + 64] = GetInput(uint3(iyx, tid.x + 64), 1);
            cache[tid.x + 96] = GetInput(uint3(iyx, tid.x + 96), 1);
        #else
            for (ic = 0; ic < InputChannels; ic += OutputChannels)
                cache[ic + tid.x] = GetInput(uint3(iyx, tid.x + ic), 1);
        #endif

            GroupMemoryBarrierWithGroupSync();

            // Transposed version of fyx
            const uint2 fyx_tr = FilterSize - 1 - fyx;

            // Calculate the product with the filter. This is also expected to
            // run in a memory coalescing fashion.
        #ifdef FINAL
            if (tid.x < OutputChannels)
        #endif
            for (ic = 0; ic < InputChannels; ic++)
                prod += GetFilter(uint4(fyx_tr, ic, tid.x)) * cache[ic];

            GroupMemoryBarrierWithGroupSync();
        }
    }

    // Output with adding the bias.
#ifdef FINAL
    if (tid.x < OutputChannels)
#endif
    StoreOutput(tid.zyx, prod + Bias[tid.x]);

#endif
}
